\documentclass[12pt]{article}

\usepackage[latin1]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{latexsym} 
\usepackage{graphicx}
\usepackage{bm}  
\usepackage{overpic} 
\usepackage[normalem]{ulem}
  
\usepackage{exscale}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage[usenames,dvipsnames]{color} % load color package
\usepackage{url}

\textwidth=6.0in \textheight=8.8in \hoffset=-0.2in
\voffset=-0.85in
\parskip=6pt
\baselineskip=9pt
\topmargin 0.8in
 
\def\black#1{\textcolor{black}{#1}}
\def\blue#1{\textcolor{blue}{#1}}
\def\red#1{\textcolor{red}{#1}}
\def\green#1{\textcolor{green}{#1}}
\def\yellow#1{\textcolor{yellow}{#1}}
\def\orange{\textcolor{BurntOrange}}

\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{corollary}{Corollary}[section]

\numberwithin{equation}{section}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\sigl}{\sigma_L}
\newcommand{\BS}{\rm BS}
\newcommand{\p}{\partial}
\newcommand{\var}{{\rm var}}
\newcommand{\cov}{{\rm cov}}
\newcommand{\beaa}{\begin{eqnarray*}}
\newcommand{\eeaa}{\end{eqnarray*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}


\def\cC{\mathcal C}
\def\cD{\mathcal D}
\def\cS{\mathcal S}
\def\cH{\mathcal H}
\def\cI{\mathcal I}
\def\cJ{\mathcal J}
\def\cL{\mathcal L}
\def\cV{\mathcal V}
\def\cR{\mathcal R}
\def\bR{\mathbb R}
\def\cX{\mathcal X}
\def\cF{\mathcal F}
\def\bP{\mathbb P}
\def\bE{\mathbb E}
\def\bN{\mathbb N}
\def\bT{\mathbb T}
\def\bC{\mathbb C}
\def\var{\text{var\,}}
\def\eps{\varepsilon}

\newcommand{\mt}{\mathbf{t}}
\newcommand{\mS}{\mathbf{S}}
\newcommand{\tC}{\widetilde{C}}
\newcommand{\hC}{\widehat{C}}
\newcommand{\tH}{\widetilde{H}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\dt}{\Delta t}
\newcommand{\tr}{{\rm tr}}

\usepackage{lipsum}
\begin{document}



\title{\bf Comparison of 3 DCC-based Covariance Matrix Estimation Methods }

\author{
Yichen Li \footnote{Department of Mathematics, Baruch College, CUNY. {\tt  yichen.li@baruch.cuny.edu}}{\setcounter{footnote}{1}} , Shuhao Liu\footnote{Department of Mathematics, Baruch College, CUNY. {\tt  shuhao.liu@baruch.cuny.edu}}
}

%\date{This version: December 25, 2011}

\maketitle\thispagestyle{empty}

%%***************************************************************************
%%
%%  Document begins here
%%
%%***************************************************************************



\begin{abstract}
We compare four unconditional covariance estimators---Sample Covariance, Linear Shrinkage (LS), Quadratic Inverse Shrinkage (QIS), and Average Oracle (AO)---both as static estimators and as priors for the Dynamic Conditional Correlation (DCC) model in large-scale portfolio optimization. Using daily returns from the 500 largest U.S. equities from 2000 to 2024, we devolatize returns via exponentially weighted moving average volatility estimates, then apply each estimator to standardized returns. The resulting unconditional covariance matrices serve as targets in the DCC recursion, producing time-varying conditional covariance matrices for global minimum-variance (GMV) portfolio construction.

Our empirical evaluation reveals that DCC-based models significantly outperform their static counterparts in unconstrained settings, with Sharpe ratios approximately doubling those of static estimators. Among the DCC variants, LS consistently delivers the strongest risk-adjusted performance, achieving an annualized Sharpe ratio of 1.58 with 81.5\% cumulative return and maximum drawdown of $-3.8$\%. QIS follows closely with a Sharpe ratio of 1.41, while the standard Sample Covariance also performs robustly within the DCC framework (Sharpe 1.33). AO exhibits the lowest volatility but underperforms with a Sharpe ratio of 1.02, largely due to aggressive rebalancing. In long-only portfolios, the performance gap narrows, though DCC-based methods still maintain a clear edge over static estimators, with cumulative returns clustering between 372\% and 391\%.

These findings suggest that accounting for dynamic correlations is more critical than the choice of unconditional prior, though LS provides the most robust foundation for DCC-based portfolio optimization. Our study represents a comprehensive comparison of these modern estimators within the DCC framework using real market data.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%  Section: Introduction
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Introduction}

Accurate estimation of covariance matrices is fundamental to portfolio optimization, risk management, and asset allocation. In high-dimensional settings where the number of assets $n$ is comparable to or exceeds the number of observations $T$, the sample covariance matrix becomes ill-conditioned and exhibits poor out-of-sample performance. This challenge is particularly acute in financial applications, where return series exhibit time-varying volatility and correlation structures.

\subsection{Problem Description}

We address the problem of estimating conditional covariance matrices for large-scale equity portfolios by comparing static covariance estimators against the Dynamic Conditional Correlation (DCC) framework of \cite{engle2002}. Specifically, we evaluate four unconditional covariance estimators---Sample Covariance, Linear Shrinkage (LS) \cite{ledoit2004}, Quadratic Inverse Shrinkage (QIS) \cite{ledoit2019}, and Average Oracle (AO) \cite{bongiorno2024}---both as standalone static estimators and as priors for the DCC model.

Our empirical evaluation uses daily returns from the 500 largest U.S. equities by market capitalization, spanning from 2000 to 2024. We first devolatize returns using exponentially weighted moving average (EWMA) volatility estimates, then apply each unconditional estimator to the standardized returns. The resulting covariance matrices serve as the unconditional correlation target $C$ in the DCC recursion, which produces time-varying conditional covariance matrices. These are subsequently used to construct global minimum-variance (GMV) portfolios, and we evaluate performance across multiple metrics including Sharpe ratio, maximum drawdown, turnover, and effective number of positions.

The key research questions are: (1) Does the dynamic modeling of correlations via DCC provide significant value over static estimators in large-scale portfolios? (2) Which unconditional covariance estimator, when embedded in the DCC framework, delivers superior risk-adjusted returns and portfolio characteristics?

\subsection{Prior literature}

The literature on covariance matrix estimation has evolved along several dimensions. Early work by \cite{ledoit2004} introduced linear shrinkage as a practical method to improve the conditioning of sample covariance matrices by shrinking eigenvalues toward their mean. This approach preserves the eigenvectors of the sample covariance while reducing estimation noise, and has become a standard benchmark in portfolio optimization.

More recently, \cite{ledoit2019} developed Quadratic Inverse Shrinkage (QIS), which applies nonlinear shrinkage in the inverse-eigenvalue domain using random matrix theory. QIS provides a consistent approximation to the oracle covariance matrix under Frobenius loss and has shown superior performance in high-dimensional settings where $n/T$ is not negligible.

The Average Oracle (AO) method of \cite{bongiorno2024} takes a different approach by directly estimating oracle eigenvalues through temporal averaging. Rather than relying on asymptotic random matrix theory, AO exploits the time series structure of financial data by computing oracle eigenvalues as the projection of future covariance matrices onto current eigenvectors, then averaging these projections over time.

For modeling time-varying correlations, \cite{engle2002} introduced the Dynamic Conditional Correlation (DCC) model, which extends univariate GARCH models to the multivariate setting. DCC decomposes the conditional covariance matrix into time-varying volatilities and a dynamic correlation matrix, allowing for parsimonious modeling of complex dependence structures. The model has been widely adopted in both academic research and industry practice.

Our contribution lies in systematically comparing these three modern unconditional estimators within the DCC framework, using a large-scale empirical study with real market data. This work represents a replication and Python implementation of the methodology presented in \cite{bongiorno2024quant}, which compares DCC-based covariance estimation methods using different unconditional priors. We provide a clean, modular implementation of the estimators and extend the empirical analysis to include additional performance metrics and robustness checks.


\newpage
\section{Model Framework and Covariance Estimators}

Let $R_t \in \mathbb{R}^n$ denote the vector of asset returns at time $t$.
Following standard practice in multivariate volatility modeling, we decompose
the conditional covariance matrix as
\begin{equation}
H_t = D_t R_t D_t,
\end{equation}
where $D_t = \mathrm{diag}(\sigma_{1,t}, \dots, \sigma_{n,t})$ is the diagonal matrix
of conditional standard deviations obtained from univariate GARCH(1,1) models, and
$R_t$ is the conditional correlation matrix.

Define the devolatilized (standardized) returns as
\begin{equation}
S_t = D_t^{-1} R_t.
\end{equation}
All unconditional covariance estimators considered below are applied to the
standardized returns $\{S_t\}_{t=1}^T$.

\subsection{Dynamic Conditional Correlation (DCC)}

The Dynamic Conditional Correlation (DCC) model of \cite{engle2002} specifies the
dynamics of the correlation matrix through an auxiliary matrix $Q_t$:
\begin{equation}
Q_t = (1-\alpha-\beta) C
      + \alpha S_t S_t^\top
      + \beta Q_{t-1},
\end{equation}
where $C$ is the unconditional covariance matrix of $S_t$, and
$\alpha, \beta \ge 0$ with $\alpha + \beta < 1$.

The correlation matrix is obtained by standardizing $Q_t$:
\begin{equation}
R_t = \mathrm{diag}(Q_t)^{-1/2} \, Q_t \, \mathrm{diag}(Q_t)^{-1/2}.
\end{equation}

Parameters $(\alpha,\beta)$ are estimated by quasi-maximum likelihood, minimizing
\begin{equation}
\sum_{t=1}^T \left(
\log \det R_t + S_t^\top R_t^{-1} S_t
\right),
\end{equation}
subject to $\alpha \ge 0$, $\beta \ge 0$, and $\alpha+\beta \le 1$.
This corresponds exactly to the implementation of the \texttt{DCCEstimator}
class, where the recursion and likelihood are evaluated sequentially.

\subsection{Linear Shrinkage (LS)}

The linear shrinkage estimator of \cite{ledoit2004} improves the conditioning
of the sample covariance matrix by shrinking its eigenvalues toward their mean.
Let
\begin{equation}
\hat C = V \Lambda V^\top
\end{equation}
be the eigen-decomposition of the sample covariance matrix of $S_t$.
The linear shrinkage estimator is defined as
\begin{equation}
\hat C^{\mathrm{LS}}
= V \left[ \rho \bar\lambda I + (1-\rho)\Lambda \right] V^\top,
\end{equation}
where $\bar\lambda = \frac{1}{n}\mathrm{tr}(\Lambda) = \frac{1}{n}\sum_{i=1}^n \lambda_i$ and $\rho \in [0,1]$
is a user-specified shrinkage intensity.
This estimator reduces estimation noise while preserving the eigenvectors of
the sample covariance matrix, as implemented in the \texttt{LS} class.

\subsection{Quadratic Inverse Shrinkage (QIS)}

Quadratic Inverse Shrinkage (QIS) is a nonlinear eigenvalue shrinkage method
derived from random matrix theory \cite{ledoit2019}.
Let $\lambda_1,\dots,\lambda_n$ denote the eigenvalues of the sample covariance
matrix of $S_t$, and define $q = n/T$.

QIS operates in the inverse-eigenvalue domain and estimates oracle-adjusted
eigenvalues $\{\delta_i\}_{i=1}^n$ via
\begin{equation}
\delta_i^{-1}
=
(1-q)^2 \lambda_i^{-1}
+ 2q(1-q)\lambda_i^{-1}\hat\theta(\lambda_i^{-1})
+ q^2 \lambda_i^{-1} A_{\hat\theta}^2(\lambda_i^{-1}),
\end{equation}
where $\hat\theta(\cdot)$ is a smoothed estimator of the Stieltjes transform
and $A_{\hat\theta}^2(\cdot)$ denotes the squared amplitude of the corresponding
analytic signal, computed using a bandwidth parameter $h = n^{-1/3}$.

The QIS covariance estimator is then given by
\begin{equation}
\hat C^{\mathrm{QIS}} = V \,\mathrm{diag}(\delta_1,\dots,\delta_n)\, V^\top.
\end{equation}
This estimator provides a consistent approximation to the oracle covariance
matrix under Frobenius loss and is implemented in the \texttt{QIS} class.

\subsection{Average Oracle (AO)}

The Average Oracle (AO) estimator \cite{bongiorno2024} directly
approximates the oracle eigenvalues by exploiting temporal information.
Let $\hat C_t$ denote the rolling sample covariance matrix of $S_t$ computed
over a lookback window of length $w$.

For each time $t$, let $\{v_{i,t}\}_{i=1}^n$ be the eigenvectors of $\hat C_t$.
The oracle eigenvalue associated with $v_{i,t}$ is defined as
\begin{equation}
d_{i,t} = v_{i,t}^\top \hat C_{t+1} v_{i,t}.
\end{equation}

The AO estimator averages these oracle eigenvalues over time:
\begin{equation}
\bar d_i = \frac{1}{T'} \sum_{t} d_{i,t},
\end{equation}
where $T'$ is the number of time points used in the averaging process.
In practice, we may sample the time indices at regular intervals to reduce
computational cost.

Given the current eigenvectors $V_t$ of $\hat C_t$, the AO covariance estimator is
\begin{equation}
\hat C^{\mathrm{AO}}_t = V_t \,\mathrm{diag}(\bar d_1,\dots,\bar d_n)\, V_t^\top.
\end{equation}
This approach bypasses explicit random matrix theory and directly estimates the
oracle spectrum through temporal averaging, as implemented in the
\texttt{AO} class with optional multiprocessing support for large datasets.

\subsection{Implementation Details and Practical Considerations}

Several implementation details are crucial for the stable integration of
unconditional covariance estimators into the DCC framework.

\paragraph{Volatility estimation and devolatization.}
The devolatilization step plays a central role in separating marginal volatility
dynamics from correlation dynamics. In our empirical study, we use a 60-day
exponentially weighted moving average (EWMA) to estimate conditional volatilities:
$\sigma_{i,t} = \sqrt{\text{EWMA}_{60}(R_{i,t}^2)} \times \sqrt{252} + 0.01$,
where the floor of 0.01 prevents division by near-zero values and the $\sqrt{252}$
factor annualizes daily volatilities. While our codebase also provides GARCH-based
devolatization via the \texttt{fit\_garch\_and\_get\_std} function, which handles
internal rescaling by dividing conditional volatilities by the model scale parameter,
the EWMA approach is computationally efficient and sufficient for large-scale
portfolios. The standardized returns $S_t = D_t^{-1} R_t$ are then used as inputs
to all unconditional covariance estimators.

\paragraph{Unconditional target consistency.}
All unconditional estimators (LS, QIS, AO) are applied to the same standardized
returns and serve exclusively as long-run targets $C$ in the DCC recursion.
Once estimated, $C$ is held fixed throughout the DCC likelihood optimization,
which uses the SLSQP algorithm to minimize the quasi-log-likelihood function
subject to constraints $\alpha \ge 0$, $\beta \ge 0$, and $\alpha + \beta \le 1$.
This design choice follows the original DCC specification and avoids repeated
re-estimation of high-dimensional covariance matrices inside the recursion,
which would otherwise introduce additional noise and computational burden.

\paragraph{DCC recursion implementation.}
The DCC update uses the outer product $\text{outer}(s_t, s_t) = s_t s_t^\top$ to
compute the rank-one update in equation (3), which is numerically more stable
than explicit matrix multiplication for sparse or near-zero standardized returns.
The correlation matrix $R_t$ is obtained by normalizing $Q_t$ using element-wise
diagonal extraction: $R_t = \text{diag}(Q_t)^{-1/2} Q_t \text{diag}(Q_t)^{-1/2}$,
where the square root and inverse operations are applied only to the diagonal elements
before broadcasting. This approach avoids full matrix inversions and maintains
numerical precision even when $Q_t$ has small off-diagonal elements.

\paragraph{Eigenstructure preservation and AO sampling.}
Both LS and QIS are rotation-invariant estimators that preserve the eigenvectors
of the sample covariance matrix while modifying its spectrum. As a result, the
conditioning and stability of the DCC updates are governed primarily by the
regularized eigenvalues rather than changes in factor directions. In contrast,
the AO estimator reconstructs the covariance matrix using rolling eigenvectors,
which makes its performance sensitive to eigenvector instability and finite-sample
effects when the lookback window is short. To mitigate computational cost, our
AO implementation supports temporal sampling (e.g., \texttt{sampling=10} processes
every 10th time point) and optional multiprocessing via the \texttt{n\_jobs}
parameter, allowing parallel evaluation of oracle eigenvalue projections across
multiple CPU cores.

\paragraph{Numerical stability and regularization.}
All estimated covariance matrices are symmetrized via $0.5 \times (C + C^\top)$
to correct for floating-point rounding errors. Small eigenvalues are clipped
at a numerical floor (e.g., $10^{-12}$ in QIS) to prevent degeneracy in matrix
inversion during portfolio optimization. In the GMV backtest, we add a ridge
regularization term $\text{ridge} \times I$ (typically $10^{-6}$) to the conditional
covariance matrix before inversion, and use the Moore--Penrose pseudoinverse
\texttt{np.linalg.pinv} to handle near-singular cases. Volatility estimates are
further clipped at $10^{-6}$ to avoid degenerate covariance slices. These safeguards
are particularly important for nonlinear shrinkage methods and rolling estimators,
where extreme eigenvalue realizations may arise in finite samples.


\newpage
\section{Numerical experiment}


\subsection{Data Description}

We ingest the daily CRSP equity data from WRDS database and clean it in several stages. Non-numeric return flags (``B'' and ``C'') in \texttt{RET} are mapped to missing values before casting the column to floating point. Observations without a valid \texttt{TICKER} or \texttt{PRC} are discarded, and market capitalization is formed as $|\text{PRC}| \times \text{SHROUT} \times (\text{CFACSHR}/\text{CFACPR})$. We retain only common shares (\texttt{SHRCD} $\in \{10,11,12\}$) to exclude preferred or foreign listings. The panel is reshaped into a \texttt{date} $\times$ \texttt{PERMNO} return matrix, and we restrict the asset universe to the 500 largest firms by market capitalization on 3 January 2000. Returns are scaled by an exponentially weighted 60-day volatility estimator multiplied by $\sqrt{252}$ with a 1\% floor, which stabilizes the subsequent conditional covariance estimation. After a 60-day burn-in, data through 1 January 2010 define the training set, while the remaining history provides the out-of-sample evaluation window. The notebook also aligns the realized volatilities with the backtest dates and clips them at $10^{-6}$ to avoid degenerate covariance slices before running the portfolio backtests.

\subsection{Metric}

Each covariance prior feeds into a Dynamic Conditional Correlation (DCC) updater that delivers a one-step conditional covariance used to rebalance a global minimum-variance (GMV) portfolio. We evaluate eight complementary metrics that are reported in Table~\ref{tab:backtest}:
\begin{itemize}
	\item \textbf{ME} --- mean daily portfolio return $\E[r_t]$.
	\item \textbf{STD} --- standard deviation of daily returns $\sqrt{\var(r_t)}$.
	\item \textbf{SR} --- annualized Sharpe ratio $\frac{\E[r_t]}{\sqrt{\var(r_t)}}\sqrt{252}$.
	\item \textbf{Return} --- total compounded performance $(\prod_t (1+r_t))-1$ over the test window.
	\item \textbf{Drawdown} --- worst peak-to-trough loss $\min_t \left(\frac{P_t}{\max_{s\le t} P_s}-1\right)$ where $P_t$ is cumulative wealth.
	\item \textbf{Turnover} --- time-average of the absolute day-over-day weight change $\|w_t-w_{t-1}\|_1$.
	\item \textbf{GrossLev} --- average gross leverage $\|w_t\|_1$ capturing total capital usage.
	\item \textbf{EffN} --- effective number of active bets, computed as $1/\sum_i w_{t,i}^2$ and averaged over time.
\end{itemize}
For the long-only variant in Table~\ref{tab:longonly_backtest}, the optimizer clips negative weights at zero and renormalizes the remaining positive allocation, leading to a fixed gross leverage of 1 while preserving the same evaluation procedure for the other statistics.

\subsection{Result}


\begin{table}[h]
\scriptsize
\centering    
\begin{tabular}{lrrrrrrrr}
\toprule
 & ME & STD & SR & Return & Drawdown & Turnover & GrossLev & EffN \\
\midrule
SampleCov & 0.000477 & 0.010610 & 0.713759 & 3.888445 & -0.315055 & 0.002821 & 4.349163 & 8.234838 \\
QIS & 0.000457 & 0.010392 & 0.698483 & 3.575675 & -0.319142 & 0.001781 & 3.777742 & 10.631438 \\
LS & 0.000454 & 0.009996 & 0.720325 & 3.582079 & -0.333852 & 0.001248 & 2.933567 & 17.668494 \\
AO & 0.000579 & 0.012913 & 0.711903 & 5.482530 & -0.398994 & 0.009329 & 8.155916 & 2.391476 \\
DCC+QIS & 0.000135 & 0.001522 & 1.412587 & 0.659802 & -0.037557 & 0.307653 & 3.814332 & 8.603517 \\
DCC+LS & 0.000159 & 0.001601 & 1.579359 & 0.815285 & -0.038311 & 0.270631 & 3.384960 & 10.803934 \\
DCC+AO & 0.000086 & 0.001334 & 1.022869 & 0.378452 & -0.031897 & 0.497708 & 5.486436 & 4.140736 \\
DCC+SampleCov & 0.000122 & 0.001456 & 1.326790 & 0.576637 & -0.035914 & 0.346738 & 4.168363 & 7.308186 \\
\bottomrule
\end{tabular}
\caption{Statistics of backtest result}
\label{tab:backtest}
\end{table}


\begin{figure}[htb!]
\begin{center}
\includegraphics[width=.8\linewidth]{8_ls.png}
\caption{Backtest result}
\label{fig:backtest}
\end{center}
\end{figure}


The most striking result in Table~\ref{tab:backtest} is the dramatic performance improvement provided by the DCC framework. While static estimators (SampleCov, QIS, LS, AO) cluster around Sharpe ratios of 0.70--0.72, their DCC counterparts achieve Sharpe ratios between 1.02 and 1.58. This confirms that accounting for time-varying correlations is far more critical than the choice of unconditional prior in unconstrained optimization.

Among the DCC variants, coupling the updater with the Ledoit--Wolf linear shrinkage prior (LS) \cite{ledoit2004} produces the strongest risk-adjusted profile. LS earns a mean daily return of 15.9 bps with 16 bps of volatility, yielding an annualized Sharpe ratio of 1.58 and an 81.5\% cumulative gain. Its maximum drawdown remains contained at $-3.8$\%, turnover averages only 0.27, and gross leverage is 3.39. Quadratic Inverse Shrinkage (QIS) trails LS modestly with a 1.41 Sharpe ratio. Notably, even the standard Sample Covariance matrix performs robustly when used as a DCC prior (DCC+SampleCov), achieving a Sharpe ratio of 1.33 and outperforming the sophisticated Average Oracle (AO) prior. DCC+AO posts the lowest Sharpe (1.02) among dynamic models, largely because the rolling lookback induces a heavier rebalancing load (turnover 0.50) and higher gross leverage (5.49).


\begin{table}[h]
\scriptsize
\centering
\begin{tabular}{lrrrrrrrr}
\toprule
 & ME & STD & SR & Return & Drawdown & Turnover & GrossLev & EffN \\
\midrule
SampleCov & 5.48 & 0.011146 & 0.781124 & 5.258330 & -0.390795 & 0.000630 & 1.000000 & 99.909685 \\
QIS & 0.000537 & 0.011006 & 0.774053 & 5.021626 & -0.387534 & 0.000473 & 1.000000 & 92.460738 \\
LS & 0.000526 & 0.010872 & 0.767450 & 4.807136 & -0.385349 & 0.000433 & 1.000000 & 95.883031 \\
AO & 0.000592 & 0.011758 & 0.799201 & 6.181936 & -0.402037 & 0.001084 & 1.000000 & 102.415808 \\
DCC+QIS & 0.000447 & 0.008109 & 0.875222 & 3.771101 & -0.313300 & 0.074744 & 1.000000 & 63.405455 \\
DCC+LS & 0.000443 & 0.007868 & 0.893133 & 3.726157 & -0.306446 & 0.073167 & 1.000000 & 64.393930 \\
DCC+AO & 0.000462 & 0.008934 & 0.820832 & 3.913599 & -0.329578 & 0.085871 & 1.000000 & 62.344215 \\
DCC+SampleCov & 0.000451 & 0.008327 & 0.860070 & 3.811869 & -0.317968 & 0.077490 & 1.000000 & 63.978533 \\
\bottomrule
\end{tabular}
\caption{Statistics of backtest result (long only)}
\label{tab:longonly_backtest}
\end{table}



\begin{figure}[htb!]
\begin{center}
\includegraphics[width=.8\linewidth]{8_lo.png}
\caption{Backtest result (long only)}
\label{fig:longonly_backtest}
\end{center}
\end{figure}


The long-only experiment in Table~\ref{tab:longonly_backtest} compresses the dispersion because the simplex constraint dominates the solution. Nevertheless, DCC-based portfolios still maintain a clear edge, achieving Sharpe ratios of 0.82--0.89 compared to 0.77--0.80 for static estimators. DCC+LS (0.89) and DCC+QIS (0.88) remain the top performers, while DCC+SampleCov (0.86) continues to perform competitively. Interestingly, the static AO estimator performs surprisingly well in this setting (Sharpe 0.80), slightly edging out other static methods. With shorts disallowed, gross leverage collapses to 1 and the effective number of positions stabilizes near the 60s for DCC models and 90s for static models, indicating that dynamic correlations encourage slightly more concentrated positions even under constraints. Overall, while the simplex constraint dampens the benefits of advanced estimation, the hierarchy of performance remains largely consistent with the unconstrained case.


\newpage
\section{Summary and conclusion}




%\appendix




The comprehensive comparison of eight covariance specifications reveals two primary insights. First, the dynamic modeling of correlations is the single most important factor for portfolio performance. Across all metrics, DCC-based models significantly outperform their static counterparts, doubling Sharpe ratios in unconstrained settings and maintaining a clear advantage in long-only portfolios. This suggests that capturing the time-varying nature of risk is more critical than the precise regularization of the unconditional covariance matrix.

Second, within the DCC framework, the Ledoit--Wolf linear shrinkage (LS) prior consistently delivers the most attractive risk-adjusted profile. In the unconstrained backtest, LS balances return and risk by pairing a modest 16 bps daily volatility with the highest Sharpe ratio (1.58) and the shallowest drawdowns. Quadratic Inverse Shrinkage (QIS) follows closely, while the standard Sample Covariance matrix also performs remarkably well (Sharpe 1.33), suggesting that the DCC recursion itself provides a form of regularization that mitigates some of the sample covariance's instability. The Average Oracle (AO) prior, although theoretically appealing, underperforms because its rolling estimation window feeds noisier covariance slices into the DCC updater. Overall, the evidence recommends LS as the robust foundation for DCC-based portfolio optimization, though even simple priors can be effective when dynamic correlations are properly modeled.

\section*{Limitations and Future Research}

Several limitations of the present study point to promising directions for
future research.

First, the Average Oracle (AO) estimator appears to be particularly sensitive
to the length of the estimation window and the stability of the eigenstructure.
While AO is theoretically appealing as a direct approximation to oracle
eigenvalues, its rolling implementation can suffer from eigenvector switching
and finite-sample noise in large but not asymptotic samples. In our implementation,
we mitigate computational cost through temporal sampling (processing every 10th
time point) and multiprocessing, but the fundamental sensitivity to window length
remains. Future work could explore hybrid variants that combine AO with additional
spectral shrinkage (e.g., applying LS or QIS to the AO-estimated eigenvalues),
subspace aggregation techniques, or adaptive window selection based on eigenvector
stability metrics to improve robustness.

Second, our analysis treats the unconditional covariance target $C$ as fixed
throughout the DCC recursion. Although this follows the standard DCC
specification and avoids computational overhead, allowing $C$ itself to evolve
slowly over time (e.g., via exponential smoothing or regime-switching models)
may further improve adaptability to long-term structural changes in correlation
regimes. Investigating time-varying or regime-dependent unconditional targets,
particularly during market stress periods, remains an open avenue for research.

Third, we focus on global minimum-variance portfolios as a clean and interpretable
benchmark that isolates the impact of covariance estimation from expected return
forecasting. Extending the comparison to expected-return-sensitive allocations
(e.g., mean-variance optimization with factor models), factor-based constraints
(e.g., sector or style neutrality), or transaction-cost-aware optimization
would provide additional insight into how covariance regularization interacts
with more complex investment objectives. The higher turnover observed for AO
suggests that transaction cost considerations may further widen the performance
gap between estimators.

Fourth, our volatility estimation uses a simple EWMA approach with a fixed
60-day window. While computationally efficient, this may not capture the full
richness of volatility dynamics compared to GARCH models or more sophisticated
volatility forecasting methods. Future work could investigate whether the relative
ranking of unconditional estimators changes when paired with different volatility
estimation techniques, or whether joint estimation of volatility and correlation
structures yields additional benefits.

Finally, while our empirical study covers a long historical sample (2000--2024)
of U.S. equities from the CRSP database, the relative performance of the
estimators may vary across asset classes, market microstructures, and sampling
frequencies. Applying the same DCC-based framework to futures, foreign exchange,
fixed income, or intraday data represents a natural extension. Additionally,
examining performance during specific market regimes (e.g., financial crises,
low-volatility periods, or high-correlation environments) could reveal
regime-dependent strengths and weaknesses of each estimator.


\section*{Acknowledgments}

% We are very grateful to Jane Brown and Janice Smith.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%  Bibliography
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{}

\bibitem{engle2002}
{Engle, R.},
{Dynamic conditional correlation: A simple class of multivariate generalized autoregressive conditional heteroskedasticity models},
{\it Journal of Business \& Economic Statistics}, \textbf{20}(3), 339--350 (2002).

\bibitem{ledoit2004}
{Ledoit, O.} and {Wolf, M.},
{A well-conditioned estimator for large-dimensional covariance matrices},
{\it Journal of Multivariate Analysis}, \textbf{88}(2), 365--411 (2004).

\bibitem{ledoit2019}
{Ledoit, O.} and {Wolf, M.},
{Quadratic shrinkage for large covariance matrices},
{\it Bernoulli}, \textbf{25}(4B), 3533--3565 (2019).

\bibitem{bongiorno2024}
{Bongiorno, E. G.} and {Challet, D.},
{Covariance matrix filtering and portfolio optimization},
{\it arXiv preprint arXiv:2111.13109} (2024).

\bibitem{bongiorno2024quant}
{Bongiorno, E. G.} and {Challet, D.},
{Comparison of DCC-based covariance matrix estimation methods},
{\it Quantitative Finance}, \textbf{24}(?), ?--? (2024).
\url{https://www.tandfonline.com/doi/full/10.1080/14697688.2024.2372053}

\bibitem{jimbook} { Gatheral, J.},
{The Volatility Surface: A Practitioner's Guide},
{Wiley Finance} (2006).

\bibitem{ghlow}
{ Gatheral, J.}, { Hsu, E.P.}, { Laurence, P.}, { Ouyang, C.}, and { Wang, T.-H.},
{Asymptotics of implied volatility in local volatility models},
{\it Mathematical Finance} (2011) forthcoming.

\end{thebibliography}

\end{document}


